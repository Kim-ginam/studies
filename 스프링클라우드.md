# UsernamePasswordAuthenticationToken
- UsernamePasswordAuthenticationToken은 Spring Security에서 사용자 인증 정보를 담는 객체입니다.
이 객체는 사용자가 로그인 화면에서 입력한 이메일(또는 사용자 이름)과 비밀번호를 포함하고 있습니다.
- 예시
``` 
new UsernamePasswordAuthenticationToken(creds.getEmail(), creds.getPassword(), new ArrayList<>());
 - creds.getEmail(): 사용자가 입력한 이메일을 나타냅니다.
 - creds.getPassword(): 사용자가 입력한 비밀번호를 나타냅니다.
 - 세 번째 인자인 new ArrayList<>()는 권한(roles)을 나타내지만, 로그인 시점에서는 비어 있습니다.
 ```
- 인증 과정
    - getAuthenticationManager().authenticate() 메서드는 실제 인증을 수행하는 부분입니다.
    - Spring Security는 AuthenticationManager를 사용하여 제공된 AuthenticationToken을 기반으로 사용자 인증을 시도합니다.
    - 이메일(또는 사용자 이름)과 비밀번호가 데이터베이스나 설정된 인증 소스와 일치하는지 확인합니다.

- 인증 결과
    - 이메일(또는 사용자 이름)과 비밀번호가 일치하면, 인증이 성공적으로 수행되고 인증된 Authentication 객체를 반환합니다.
    - 일치하지 않으면, BadCredentialsException과 같은 예외가 발생하여 인증 실패를 알립니다.


# UserDetailsService 구현 예시
```
@Override
public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException {
    UserEntity userEntity = userRepository.findByEmail(username);

    if (userEntity == null)
        throw new UsernameNotFoundException(username + ": not found");

    return new User(userEntity.getEmail(), userEntity.getEncryptedPwd(),
            true, true, true, true,
            new ArrayList<>());
}

 - 사용자 엔티티를 찾아 UserDetails 객체로 변환하여 반환합니다.
 - 비밀번호 비교는 getAuthenticationManager().authenticate()에서 처리됩니다.
```
# OpenFeign vs RestTemplate
- OpenFeign
    - 각 마이크로서비스를 알고 있을 때 사용하기 적합합니다.
    - 직관적이고, 자바 호출처럼 사용이 가능합니다.
```
@FeignClient(name="catalog-service", configuration = FeignErrorDecoder2.class)
public interface CatalogServiceClient {

    @GetMapping("/catalog-service/getCatalogs_wrong")
    List<ResponseCatalog> getCatalogs();

}
```
- RestTemplate
    - 환경 설정에 정의된 IP를 기반으로 URL과 HTTP 메서드를 사용하여 데이터를 전송하고, getBody를 통해 응답 데이터를 받습니다.
    - 전통적으로 많이 사용된 방식입니다.
    
```
    @Override
    public String getAroundHub() {
        //uri는 어떤경로로 요청을 할건지해서 사용하는 클래스
        URI uri= UriComponentsBuilder
                .fromUriString("httpL//localhost:8081")
                .path("/api/server/around-hub")
                .encode() //기본적 utf-8로
                .build() //return 값 uricomponent 이기때문에
                .toUri(); //uri로 바꿔줌

        RestTemplate restTemplate=new RestTemplate();
        // 응답을 ResponseEntity 객체로 받는다. getForObject()와 달리 HTTP 응답에대한
        // 추가 정보를 담고 있어서 GET 요청에 대한 응답 코드, 실제 데이터를 확인할 수 있다.
        // 또한 ResponseEntity<T> 제네릭 타입에 따라서 응답을 String이나 Object 객체로 받을 수 있다.
        ResponseEntity<String> responseEntity=restTemplate.getForEntity(uri, String.class); //만들어진 uri값과 그 타입과 맞춘 클래스

        LOGGER.info("status code: {}", responseEntity.getStatusCode());
        LOGGER.info("body : {}", responseEntity.getBody());

        return responseEntity.getBody();
    }
 - restTemplate.getForEntity(serverUrl, String.class)는 지정된 URL로 GET 요청을 보내고, 응답을 ResponseEntity 형태로 받습니다.
 - getBody() 메서드를 통해 실제 응답 데이터를 얻을 수 있습니다.
```

- OpenFeign의 장점
    - 로그를 통해 호출 과정을 볼 수 있으며, FeignException을 통해 에러 발생 시 해당 부분만 제외하고 나머지 작업을 계속 진행할 수 있습니다.

## ErrorDecoder
- ErrorDecoder를 통해 Feign 클라이언트에서 발생한 에러를 상태 코드 값에 따라 분기 처리할 수 있습니다.
```
@Bean
public FeignErrorDecoder getFeignErrorDecoder() {
    return new FeignErrorDecoder();
}

@Override
public Exception decode(String methodKey, Response response) {
    switch(response.status()) {
        case 400:
            break;
        case 404:
            if (methodKey.contains("getOrders")) {
                return new ResponseStatusException(HttpStatus.valueOf(response.status()),
                       "User's orders is empty");
            }
            break;
        default:
            return new Exception(response.reason());
    }

    return null;
}
 - 환경 변수(env)를 사용하여 에러 메시지를 출력할 수 있습니다.
 - Config 정보가 외부에 있을 경우, 이를 변경하고 Spring Cloud Bus를 통해 리프레시 할 수 있습니다.
```

# Kafka를 이용한 데이터베이스 동기화 및 아키텍처 구성
- 서비스 데이터 동기화 문제
    - 동일한 서비스를 수평적으로 확장할 때 데이터베이스에 저장되는 데이터의 동기화 문제가 발생할 수 있습니다.
    - 데이터 동기화 문제를 해결하는 방법:
        - 하나의 데이터베이스 사용: 트랜잭션 관리를 통해 데이터 일관성을 유지.
        - 데이터베이스 동기화: Apache Kafka를 사용하여 데이터를 동기화.
        - Kafka + 단일 데이터베이스: 서비스 -> Kafka -> 데이터베이스로 데이터 흐름을 관리.

## Kafka를 이용한 데이터 흐름
- Kafka를 사용하여 실시간 데이터 피드를 관리하고 높은 처리량과 낮은 지연 시간을 제공합니다.
- 데이터 전송에 있어 전송하는 쪽은 Producer, 데이터를 받는 쪽은 Consumer로 역할을 나눕니다.
- Kafka는 여러 대의 서버(클러스터)로 구성하며, Zookeeper를 사용하여 서버 간의 코디네이터 역할을 수행합니다.

## Kafka 설치 및 실행
- Zookeeper 실행
```
C:\kafka>bin\windows\zookeeper-server-start.bat config\zookeeper.properties
```
- Kafka 실행
```
C:\kafka>bin\windows\kafka-server-start.bat config\server.properties
```
- 메시지 생산 (Producer)
```
C:\kafka>bin\windows\kafka-console-producer.bat --broker-list localhost:9092 --topic quickstart-events
```
- 메시지 소비 (Consumer)
```
C:\kafka>bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic quickstart-events
```

## Kafka Connect를 이용한 데이터 이동
- Kafka Connect를 사용하여 코드 작성 없이 데이터를 임포트 및 익스포트할 수 있습니다.
- 데이터 이동 아키텍처
```
소스 시스템 -> Kafka Connect Source -> Kafka 클러스터 -> Kafka Connect Sink -> 타겟 시스템
```

## Kafka와 MariaDB 연동 설정
- MariaDB 설치: mariadb.org
- Kafka Connect 설치: Confluent Kafka Connect
- JDBC 커넥터 설치: Confluent JDBC Connector

## Kafka Connect 설정 예시
- connect-distributed.properties 파일에서 jdbc 경로 설정
```
plugin.path=C\\confluentinc-kafka-connect-jdbc-10.7.12\\lib
```
## Source Connector 설정
- Source Connector를 설정하여 MariaDB에서 데이터를 읽어 Kafka로 전송
```
{
    "name" : "my-source-connect",
    "config" : {
        "connector.class" : "io.confluent.connect.jdbc.JdbcSourceConnector",
        "connection.url":"jdbc:mysql://localhost:3307/mydb",
        "connection.user":"root",
        "connection.password":"test1357",
        "mode": "incrementing",
        "incrementing.column.name" : "id",
        "table.whitelist":"users", 
        "topic.prefix" : "my_topic_",
        "tasks.max" : "1"
    }
}
```
## Sink Connector 설정
- Sink Connector를 설정하여 Kafka 데이터를 타겟 시스템으로 전송
```
{
    "name":"my-sink-connect",
    "config":{
        "connector.class":"io.confluent.connect.jdbc.JdbcSinkConnector",
        "connection.url":"jdbc:mysql://localhost:3307/mydb",
        "connection.user":"root",
        "connection.password":"test1357",
        "auto.create":"true",
        "auto.evolve":"true",
        "delete.enabled":"false",
        "tasks.max":"1",
        "topics":"my_topic_users"
    }
}
```
## Kafka Producer와 Consumer 설정
- Producer 설정
```
@Bean
public ProducerFactory<String, String> producerFactory() {
    Map<String, Object> properties = new HashMap<>();
    properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "127.0.0.1:9092");
    properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
    properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
    return new DefaultKafkaProducerFactory<>(properties);
}
```
## Consumer 설정
```
@Bean
public ConsumerFactory<String, String> consumerFactory() {
    Map<String, Object> properties = new HashMap<>();
    properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "127.0.0.1:9092");
    properties.put(ConsumerConfig.GROUP_ID_CONFIG, "consumerGroupId");
    properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
    properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
    properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
    return new DefaultKafkaConsumerFactory<>(properties);
}
```
## Kafka Data Transfer Object (DTO) 클래스 예시
- KafkaOrderDto 클래스 정의
```
@Data
@AllArgsConstructor
public class KafkaOrderDto implements Serializable {
    private Schema schema;
    private Payload payload;
}
```
- Schema 클래스
```
@Data
@Builder
public class Schema {
    private String type;
    private List<Field> fields;
    private boolean optional;
    private String name;
}
```
- Field 클래스
```
@Data
@AllArgsConstructor
public class Field {
    private String type;
    private boolean optional;
    private String field;
}
```
- Payload 클래스
```
@Data
@Builder
public class Payload {
    private String order_id;
    private String user_id;
    private String product_id;
    private int qty;
    private int unit_price;
    private int total_price;
}
```
## 데이터 전송 예시
- OrderProducer 클래스에서 데이터를 Kafka로 전송
```
@EnableKafka
@Configuration
public class KafkaProducerConfig {
    @Bean
    public ProducerFactory<String, String> producerFactory() {
        Map<String, Object> properties = new HashMap<>();
        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "127.0.0.1:9092");
        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);

        return new DefaultKafkaProducerFactory<>(properties);
    }

    @Bean
    public KafkaTemplate<String, String> kafkaTemplate() {
        return new KafkaTemplate<>(producerFactory());
    }
}
```
```
@Autowired
public OrderProducer(KafkaTemplate<String, String> kafkaTemplate) {
    this.kafkaTemplate = kafkaTemplate;
}

public OrderDto send(String topic, OrderDto orderDto) {
    Payload payload = Payload.builder()
            .order_id(orderDto.getOrderId())
            .user_id(orderDto.getUserId())
            .product_id(orderDto.getProductId())
            .qty(orderDto.getQty())
            .unit_price(orderDto.getUnitPrice())
            .total_price(orderDto.getTotalPrice())
            .build();

    KafkaOrderDto kafkaOrderDto = new KafkaOrderDto(schema, payload);
    ObjectMapper mapper = new ObjectMapper();
    String jsonInString = "";

    try {
        jsonInString = mapper.writeValueAsString(kafkaOrderDto);
    } catch(JsonProcessingException ex) {
        ex.printStackTrace();
    }

    kafkaTemplate.send(topic, jsonInString);
}
```

## 아키텍처 구성

- 주요 구성 요소
    - 오더 서비스 -> Kafka -> 데이터베이스 순서로 데이터 전송
    - Kafka를 사용하여 Producer에서 데이터를 큐잉하고, Consumer가 이를 수신하여 데이터베이스에 저장
- @EnableKafka: Kafka 사용 설정
- @KafkaListener: 특정 토픽을 리스닝하여 데이터를 수신

# 서킷브레이커 (Circuit Breaker)
- 마이크로서비스 간의 통신 중 오류가 발생할 경우 연쇄적인 오류를 방지하기 위해 우회로 설정을 해주는 패턴.
- 서킷브레이커는 정상 상태에서 닫혀있다가, 오류 발생 시 열리며, 일정 시간 후 반개방(half-open) 상태가 됩니다.
- 서킷브레이커 설정 예시
```
CircuitBreakerConfig circuitBreakerConfig = CircuitBreakerConfig.custom()
    .failureRateThreshold(4) // 서킷브레이커를 열지 결정하는 실패 확률 설정
    .waitDurationInOpenState(Duration.ofMillis(1000)) // Open 상태를 유지하는 시간 설정
    .slidingWindowType(CircuitBreakerConfig.SlidingWindowType.COUNT_BASED) // 호출 결과를 시간 기반 또는 카운트 기반으로 설정
    .slidingWindowSize(2) // 슬라이딩 창의 크기 설정
    .build();

- 타임리미터 설정 예시

TimeLimiterConfig timeLimiterConfig = TimeLimiterConfig.custom()
    .timeoutDuration(Duration.ofSeconds(4)) // 서비스 응답 시간 초과 제한 설정
    .build();

return factory -> factory.configureDefault(id -> new Resilience4JConfigBuilder(id)
                .timeLimiterConfig(timeLimiterConfig)
                .circuitBreakerConfig(circuitBreakerConfig)
                .build()
        );
```
- 사용 예시
```
ordersList = orderServiceClient.getOrders(userId);
CircuitBreaker circuitBreaker = circuitBreakerFactory.create("circuitBreaker1");
CircuitBreaker circuitBreaker2 = circuitBreakerFactory.create("circuitBreaker2");
ordersList = circuitBreaker.run(() -> orderServiceClient.getOrders(userId),
             throwable -> new ArrayList<>());

```
## Zipkin과 Spring Cloud Sleuth
- Zipkin: 분산 환경에서 시스템 병목현상 파악을 위해 사용.
- Spring Cloud Sleuth: Zipkin과 연동하여 로그 파일이나 스트림 데이터를 Zipkin에 전달.
- 주요 용어
    - trace ID: 요청이 시작된 ID.
    - span ID: 서비스 호출 ID.
## Zipkin 의존성 추가
```
<dependency>  
    <groupId>org.springframework.cloud</groupId> 
    <artifactId>spring-cloud-starter-sleuth</artifactId> 
    <version>3.1.11</version> 
</dependency> 

<dependency>
    <groupId>io.micrometer</groupId>
    <artifactId>micrometer-observation</artifactId>
</dependency>

<dependency>
    <groupId>io.zipkin.brave</groupId>
    <artifactId>brave-instrumentation-spring-web</artifactId>
</dependency>

<dependency>
    <groupId>io.zipkin.reporter2</groupId>
    <artifactId>zipkin-reporter-brave</artifactId>
</dependency>
```
## Zipkin 설정
```
zipkin:
  base-url: http://localhost:9411
  enabled: true
```
## 로그에서 확인
- @Slf4j를 사용하여 로그를 남기면 trace ID, span ID가 표시됨.
- Zipkin에 trace ID를 입력하면 서비스의 동작 흐름을 시각적으로 확인 가능.

## 마이크로미터 (Micrometer)
- 자바 기반 애플리케이션 메트릭 제공 라이브러리.
- 스프링의 메트릭 처리 및 프로메테우스, 그라파나 같은 모니터링 시스템과 통합 가능.

## Spring Actuator
Spring Actuator: 애플리케이션 상태와 성능을 쉽게 확인하고 관리할 수 있는 다양한 엔드포인트 제공.
```
application.yaml

  endpoints:
    web:
      exposure:
        include: health, httptrace, info, metrics, prometheus
```
# 프로메테우스 (Prometheus)와 그라파나 (Grafana)
## 프로메테우스
- 매트릭을 수집하고 모니터링 및 알람에 사용되는 오픈소스 애플리케이션.
- Prometheus 서버가 데이터를 수집할 타겟을 prometheus.yml 파일에서 설정.
## 그라파나
- 수집된 데이터를 시각화하는 애플리케이션.

## Prometheus 설정 예시 (prometheus.yml)
```
- job_name: 'user-service'
  scrape_interval: 15s
  metrics_path: '/user-service/actuator/prometheus'
  static_configs:
    - targets: ['localhost:8000']

- job_name: 'order-service'
  scrape_interval: 15s
  metrics_path: '/order-service/actuator/prometheus'
  static_configs:
    - targets: ['localhost:8001']

- job_name: 'apigateway-service'
  scrape_interval: 15s
  metrics_path: '/actuator/prometheus'
  static_configs:
    - targets: ['localhost:8002']

- metrics_path는 모니터링할 엔드포인트를 지정하며, scrape_interval은 데이터를 수집하는 주기를 의미.
```
## 프로메테우스와 그라파나 연동
- 프로메테우스에서 수집한 메트릭 데이터를 그라파나에서 시각화하여 모니터링.
- 그라파나 대시보드에서 시각적으로 메트릭을 확인하고 문제 발생 시 경고 알람 설정 가능.
